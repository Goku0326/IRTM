{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c810f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Gokul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk #Import NLTK library\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import string\n",
    "nltk.download('punkt') #installed punkt to fix error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a837508e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#reading each text file\n",
    "with open(\"C:/Users/Gokul/Documents/IRTM_Project/Books_new/Ponniyin_Selvan_-_Kalki.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    file1 = file.read()\n",
    "with open(\"C:/Users/Gokul/Documents/IRTM_Project/Books_new/Whirlwind_part2.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    file2 = file.read()\n",
    "with open(\"C:/Users/Gokul/Documents/IRTM_Project/Books_new/ponniyin-selvan-the-killer-sword-part-3.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    file3 = file.read()\n",
    "with open(\"C:/Users/Gokul/Documents/IRTM_Project/Books_new/ponniyin-selvan-the-crown-part-4.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    file4 = file.read()\n",
    "with open(\"C:/Users/Gokul/Documents/IRTM_Project/Books_new/ponniyin-selvan-the-supreme-sacrifice-part-5.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    file5 = file.read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5938b8",
   "metadata": {},
   "source": [
    "### Combined File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc0066b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = file1+file2+file3+file4+file5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65417d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7cf357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b08902b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3690376"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98d05378",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.words('english')) #English stopwords assigned to \"stopwords\" object\n",
    "\n",
    "import string #Punctuation\n",
    "def remove_punctuations(text):\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, '')\n",
    "    return text\n",
    "\n",
    "stopwords = [''.join(item for item in x if item not in string.punctuation) for x in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da87ccc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming file_content1 is a string containing the text you want to process\n",
    "stopwords = set(stopwords)  # Convert stopwords to a set for efficient lookup\n",
    "\n",
    "# Split the string into individual words\n",
    "words = file.split()\n",
    "\n",
    "# Remove stopwords from the list of words\n",
    "filtered_words = [word for word in words if word not in stopwords]\n",
    "\n",
    "# Join the filtered words back into a string\n",
    "filtered_file = ' '.join(filtered_words)\n",
    "\n",
    "# Now you can use filtered_content1, which is the string with stopwords removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15c6f778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2634869"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e625a9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = file.replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bcce3dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2312f339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22891ca7a6274b4481bdc9880c94080c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gokul\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Gokul\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "374a0409623e489ea61a4061dbc85216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b08fffaa976741a48d02ffcc7ee2c90c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37eaf96c4a1946db87eb8dc718127819",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f4e11351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence tokenization\n",
    "sentences = nltk.sent_tokenize(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e70c3782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We shall cross a century in a convenient second, and reach a year roughly nine hundred and eighty two years before our time.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9f6f6ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the batch size\n",
    "batch_size = 3\n",
    "\n",
    "# Formulate the batch_sentences\n",
    "batch_sentences = []\n",
    "for i in range(0, len(sentences), batch_size):\n",
    "    batch = sentences[i : i + batch_size]\n",
    "    batch_sentences.append(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f84777e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sentences = [sentence.strip() for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d2564f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_inputs = tokenizer(batch_sentences, padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "816bcf24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(encoded_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7eb8900a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = encoded_inputs['input_ids']\n",
    "attention_mask = encoded_inputs['attention_mask']\n",
    "token_type_ids = encoded_inputs.get('token_type_ids')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b675246b",
   "metadata": {},
   "source": [
    "#### Build Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d685fb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dc3a015d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = encoded_input['input_ids']\n",
    "attention_mask = encoded_input['attention_mask']\n",
    "token_type_ids = encoded_input.get('token_type_ids')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a1376f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cef8516c961049ed919c6cff976d9f6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac5b124de70e48f19c217cc46f668785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9413330fcfbe4d53aa7fa014d43130fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b906dcdb7af94f96ac146326967c5fae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the sentiment analysis pipeline\n",
    "nlp = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "38628a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total positive sentences: 24083\n",
      "Total negative sentences: 28558\n"
     ]
    }
   ],
   "source": [
    "# Initialize counters\n",
    "positive_count = 0\n",
    "negative_count = 0\n",
    "\n",
    "# Process each sentence and update counters\n",
    "for sentence in sentences:\n",
    "    result = nlp(sentence)[0]  # Perform sentiment analysis\n",
    "    label = result['label']  # Get the sentiment label\n",
    "\n",
    "    if label == \"POSITIVE\":\n",
    "        positive_count += 1\n",
    "    elif label == \"NEGATIVE\":\n",
    "        negative_count += 1\n",
    "\n",
    "# Calculate totals\n",
    "total_positive_sentences = positive_count\n",
    "total_negative_sentences = negative_count\n",
    "\n",
    "# Print the results\n",
    "print(\"Total positive sentences:\", total_positive_sentences)\n",
    "print(\"Total negative sentences:\", total_negative_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "21ee8d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total positive sentences: 24083\n",
      "Total negative sentences: 28558\n",
      "\n",
      "Top 5 positive sentences:\n",
      "Sentence: People milled around all of them, enjoying the sights and scenes.\n",
      "Score: 0.9999\n",
      "\n",
      "Sentence: This one has an excellent history, and worthy of worship, as well.\n",
      "Score: 0.9999\n",
      "\n",
      "Sentence: Welcome, welcome, to Rains, Wealth and Fortune!]\n",
      "Score: 0.9999\n",
      "\n",
      "Sentence: The land shall flourish!\n",
      "Score: 0.9999\n",
      "\n",
      "Sentence: I'm impressed.\n",
      "Score: 0.9999\n",
      "\n",
      "\n",
      "Top 5 negative sentences:\n",
      "Sentence: You must be very tired.\n",
      "Score: 0.9998\n",
      "\n",
      "Sentence: “Really, I fail to the see the point of these petty arguments.\n",
      "Score: 0.9998\n",
      "\n",
      "Sentence: This comprehensively rude argument did not seem to have any effect on the sanyasi.\n",
      "Score: 0.9998\n",
      "\n",
      "Sentence: But ah—what a disappointment!\n",
      "Score: 0.9998\n",
      "\n",
      "Sentence: You needn't entertain any suspicions on that score.\n",
      "Score: 0.9998\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize counters\n",
    "positive_count = 0\n",
    "negative_count = 0\n",
    "\n",
    "# Create lists to store positive and negative sentence results\n",
    "positive_sentences = []\n",
    "negative_sentences = []\n",
    "\n",
    "#Process each sentence and update counters\n",
    "for sentence in sentences:\n",
    "    result = nlp(sentence)[0]  # Perform sentiment analysis\n",
    "    label = result['label']  # Get the sentiment label\n",
    "    score = round(result['score'], 4)  # Get the sentiment score\n",
    "\n",
    "    if label == \"POSITIVE\":\n",
    "        positive_count += 1\n",
    "        positive_sentences.append((sentence, score))\n",
    "    elif label == \"NEGATIVE\":\n",
    "        negative_count += 1\n",
    "        negative_sentences.append((sentence, score))\n",
    "\n",
    "# Sort positive and negative sentences by score (in descending order)\n",
    "positive_sentences.sort(key=lambda x: x[1], reverse=True)\n",
    "negative_sentences.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the results\n",
    "print(\"Total positive sentences:\", positive_count)\n",
    "print(\"Total negative sentences:\", negative_count)\n",
    "print(\"\\nTop 5 positive sentences:\")\n",
    "for sentence, score in positive_sentences[:5]:\n",
    "    print(f\"Sentence: {sentence}\\nScore: {score}\\n\")\n",
    "print(\"\\nTop 5 negative sentences:\")\n",
    "for sentence, score in negative_sentences[:5]:\n",
    "    print(f\"Sentence: {sentence}\\nScore: {score}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24c7d84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flair\n",
      "  Downloading flair-0.12.2-py3-none-any.whl (373 kB)\n",
      "     -------------------------------------- 373.1/373.1 kB 7.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm>=4.26.0 in c:\\users\\gokul\\anaconda3\\lib\\site-packages (from flair) (4.64.1)\n",
      "Collecting deprecated>=1.2.4\n",
      "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.10.0 in c:\\users\\gokul\\anaconda3\\lib\\site-packages (from flair) (0.13.4)\n",
      "Requirement already satisfied: regex in c:\\users\\gokul\\anaconda3\\lib\\site-packages (from flair) (2022.7.9)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\gokul\\anaconda3\\lib\\site-packages (from flair) (2.8.2)\n",
      "Collecting segtok>=1.5.7\n",
      "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: gensim>=3.8.0 in c:\\users\\gokul\\anaconda3\\lib\\site-packages (from flair) (4.1.2)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in c:\\users\\gokul\\anaconda3\\lib\\site-packages (from flair) (1.0.2)\n",
      "Requirement already satisfied: lxml in c:\\users\\gokul\\anaconda3\\lib\\site-packages (from flair) (4.9.1)\n",
      "Requirement already satisfied: langdetect in c:\\users\\gokul\\anaconda3\\lib\\site-packages (from flair) (1.0.9)\n",
      "Collecting transformer-smaller-training-vocab>=0.2.1\n",
      "  Downloading transformer_smaller_training_vocab-0.2.4-py3-none-any.whl (13 kB)\n",
      "Collecting pptree\n",
      "  Downloading pptree-3.1.tar.gz (3.0 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: more-itertools in c:\\users\\gokul\\anaconda3\\lib\\site-packages (from flair) (9.1.0)\n",
      "Collecting gdown==4.4.0\n",
      "  Downloading gdown-4.4.0.tar.gz (14 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting hyperopt>=0.2.7\n",
      "  Downloading hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n",
      "     ---------------------------------------- 1.6/1.6 MB 4.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tabulate in c:\\users\\gokul\\anaconda3\\lib\\site-packages (from flair) (0.8.10)\n",
      "Collecting conllu>=4.0\n",
      "  Downloading conllu-4.5.2-py2.py3-none-any.whl (16 kB)\n",
      "Collecting pytorch-revgrad\n",
      "  Downloading pytorch_revgrad-0.2.0-py3-none-any.whl (4.6 kB)\n",
      "Collecting bpemb>=0.3.2\n",
      "  Downloading bpemb-0.3.4-py3-none-any.whl (19 kB)\n",
      "Collecting ftfy\n",
      "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
      "     ---------------------------------------- 53.1/53.1 kB 2.7 MB/s eta 0:00:00\n",
      "Collecting mpld3==0.3\n",
      "  Downloading mpld3-0.3.tar.gz (788 kB)\n",
      "     -------------------------------------- 788.5/788.5 kB 4.5 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: transformers[sentencepiece]>=4.18.0 in c:\\users\\gokul\\anaconda3\\lib\\site-packages (from flair) (4.28.1)\n",
      "Collecting sqlitedict>=1.6.0\n",
      "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting janome\n",
      "  Downloading Janome-0.4.2-py2.py3-none-any.whl (19.7 MB)\n",
      "     --------------------------------------- 19.7/19.7 MB 10.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in c:\\users\\gokul\\anaconda3\\lib\\site-packages (from flair) (3.5.2)\n",
      "Requirement already satisfied: boto3 in c:\\users\\gokul\\anaconda3\\lib\\site-packages (from flair) (1.24.28)\n",
      "Collecting wikipedia-api\n",
      "  Downloading Wikipedia_API-0.5.8-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: torch!=1.8,>=1.5.0 in c:\\users\\gokul\\anaconda3\\lib\\site-packages (from flair) (2.0.0)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\gokul\\anaconda3\\lib\\site-packages (from gdown==4.4.0->flair) (2.28.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\gokul\\anaconda3\\lib\\site-packages (from gdown==4.4.0->flair) (3.6.0)\n",
      "Requirement already satisfied: six in c:\\users\\gokul\\anaconda3\\lib\\site-packages (from gdown==4.4.0->flair) (1.16.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\gokul\\anaconda3\\lib\\site-packages (from gdown==4.4.0->flair) (4.11.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\gokul\\anaconda3\\lib\\site-packages (from bpemb>=0.3.2->flair) (1.21.5)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\gokul\\anaconda3\\lib\\site-packages (from bpemb>=0.3.2->flair) (0.1.98)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\gokul\\anaconda3\\lib\\site-packages (from deprecated>=1.2.4->flair) (1.14.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\gokul\\anaconda3\\lib\\site-packages (from gensim>=3.8.0->flair) (5.2.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\gokul\\anaconda3\\lib\\site-packages (from gensim>=3.8.0->flair) (1.9.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\gokul\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.10.0->flair) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\gokul\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.10.0->flair) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\gokul\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.10.0->flair) (4.3.0)\n",
      "Collecting py4j\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "     ------------------------------------- 200.5/200.5 kB 12.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\gokul\\anaconda3\\lib\\site-packages (from hyperopt>=0.2.7->flair) (2.0.0)\n",
      "Requirement already satisfied: future in c:\\users\\gokul\\anaconda3\\lib\\site-packages (from hyperopt>=0.2.7->flair) (0.18.2)\n",
      "Requirement already satisfied: networkx>=2.2 in c:\\users\\gokul\\anaconda3\\lib\\site-packages (from hyperopt>=0.2.7->flair) (2.8.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\gokul\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (9.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\gokul\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (1.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\gokul\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\gokul\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\gokul\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (4.25.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\gokul\\anaconda3\\lib\\site-packages (from scikit-learn>=0.21.3->flair) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\gokul\\anaconda3\\lib\\site-packages (from scikit-learn>=0.21.3->flair) (1.1.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\gokul\\anaconda3\\lib\\site-packages (from torch!=1.8,>=1.5.0->flair) (1.10.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\gokul\\anaconda3\\lib\\site-packages (from torch!=1.8,>=1.5.0->flair) (2.11.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\gokul\\anaconda3\\lib\\site-packages (from tqdm>=4.26.0->flair) (0.4.5)\n",
      "Collecting datasets<3.0.0,>=2.0.0\n",
      "  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n",
      "     -------------------------------------- 474.6/474.6 kB 7.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\gokul\\anaconda3\\lib\\site-packages (from transformers[sentencepiece]>=4.18.0->flair) (0.13.3)\n",
      "Collecting protobuf<=3.20.2\n",
      "  Downloading protobuf-3.20.2-cp39-cp39-win_amd64.whl (904 kB)\n",
      "     -------------------------------------- 904.2/904.2 kB 9.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\gokul\\anaconda3\\lib\\site-packages (from boto3->flair) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in c:\\users\\gokul\\anaconda3\\lib\\site-packages (from boto3->flair) (0.6.0)\n",
      "Requirement already satisfied: botocore<1.28.0,>=1.27.28 in c:\\users\\gokul\\anaconda3\\lib\\site-packages (from boto3->flair) (1.27.28)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in c:\\users\\gokul\\anaconda3\\lib\\site-packages (from ftfy->flair) (0.2.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in c:\\users\\gokul\\anaconda3\\lib\\site-packages (from botocore<1.28.0,>=1.27.28->boto3->flair) (1.26.11)\n",
      "Collecting pyarrow>=8.0.0\n",
      "  Downloading pyarrow-12.0.0-cp39-cp39-win_amd64.whl (21.5 MB)\n",
      "     --------------------------------------- 21.5/21.5 MB 10.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pandas in c:\\users\\gokul\\anaconda3\\lib\\site-packages (from datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (1.4.4)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in c:\\users\\gokul\\anaconda3\\lib\\site-packages (from datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (0.3.4)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
      "     -------------------------------------- 132.9/132.9 kB 7.7 MB/s eta 0:00:00\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.8.4-cp39-cp39-win_amd64.whl (323 kB)\n",
      "     ------------------------------------- 323.6/323.6 kB 10.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\gokul\\anaconda3\\lib\\site-packages (from datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (2022.7.1)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.2.0-cp39-cp39-win_amd64.whl (30 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gokul\\anaconda3\\lib\\site-packages (from requests[socks]->gdown==4.4.0->flair) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\gokul\\anaconda3\\lib\\site-packages (from requests[socks]->gdown==4.4.0->flair) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gokul\\anaconda3\\lib\\site-packages (from requests[socks]->gdown==4.4.0->flair) (2022.9.14)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\gokul\\anaconda3\\lib\\site-packages (from beautifulsoup4->gdown==4.4.0->flair) (2.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\gokul\\anaconda3\\lib\\site-packages (from jinja2->torch!=1.8,>=1.5.0->flair) (2.0.1)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\gokul\\anaconda3\\lib\\site-packages (from requests[socks]->gdown==4.4.0->flair) (1.7.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\gokul\\anaconda3\\lib\\site-packages (from sympy->torch!=1.8,>=1.5.0->flair) (1.2.1)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.9.2-cp39-cp39-win_amd64.whl (61 kB)\n",
      "     ---------------------------------------- 61.7/61.7 kB ? eta 0:00:00\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.4-cp39-cp39-win_amd64.whl (28 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\gokul\\anaconda3\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (21.4.0)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.3-cp39-cp39-win_amd64.whl (34 kB)\n",
      "Collecting dill<0.3.7,>=0.3.0\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "     -------------------------------------- 110.5/110.5 kB 6.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\gokul\\anaconda3\\lib\\site-packages (from pandas->datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (2023.3)\n",
      "Building wheels for collected packages: gdown, mpld3, sqlitedict, pptree\n",
      "  Building wheel for gdown (pyproject.toml): started\n",
      "  Building wheel for gdown (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for gdown: filename=gdown-4.4.0-py3-none-any.whl size=14786 sha256=163e97e83d08291db5155a9384bdc851b22aeffc1497fe39024ed1c5cc5a4ad8\n",
      "  Stored in directory: c:\\users\\gokul\\appdata\\local\\pip\\cache\\wheels\\7d\\37\\b6\\b2a79c75e898c0b8e46ff255102602d7159a10d9af0d80641a\n",
      "  Building wheel for mpld3 (setup.py): started\n",
      "  Building wheel for mpld3 (setup.py): finished with status 'done'\n",
      "  Created wheel for mpld3: filename=mpld3-0.3-py3-none-any.whl size=116686 sha256=fe457a85cd7080f7768c1bc77e775b2599ddcca0b8efd95260485dba50a7b332\n",
      "  Stored in directory: c:\\users\\gokul\\appdata\\local\\pip\\cache\\wheels\\a6\\f4\\e6\\e40ff9021f6b3854af70fa8ea004f5ab95672817462df08fed\n",
      "  Building wheel for sqlitedict (setup.py): started\n",
      "  Building wheel for sqlitedict (setup.py): finished with status 'done'\n",
      "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16864 sha256=2ab4a3156ed875040c58df21cdcce490609c1f5ec879afe004d90c17f7253ee3\n",
      "  Stored in directory: c:\\users\\gokul\\appdata\\local\\pip\\cache\\wheels\\f6\\48\\c4\\942f7a1d556fddd2348cb9ac262f251873dfd8a39afec5678e\n",
      "  Building wheel for pptree (setup.py): started\n",
      "  Building wheel for pptree (setup.py): finished with status 'done'\n",
      "  Created wheel for pptree: filename=pptree-3.1-py3-none-any.whl size=4609 sha256=b0ab5e87cadd63b0f4672d608a7a5be80228c174e78906ccff8115c7bb30364c\n",
      "  Stored in directory: c:\\users\\gokul\\appdata\\local\\pip\\cache\\wheels\\52\\0e\\51\\514e690004ea9713bc3fdb678d5e2768fcc597d0c3b6a3abd2\n",
      "Successfully built gdown mpld3 sqlitedict pptree\n",
      "Installing collected packages: sqlitedict, py4j, pptree, mpld3, janome, xxhash, segtok, pyarrow, protobuf, multidict, ftfy, frozenlist, dill, deprecated, conllu, async-timeout, yarl, wikipedia-api, responses, multiprocess, hyperopt, aiosignal, pytorch-revgrad, gdown, bpemb, aiohttp, datasets, transformer-smaller-training-vocab, flair\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.4\n",
      "    Uninstalling dill-0.3.4:\n",
      "      Successfully uninstalled dill-0.3.4\n",
      "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 bpemb-0.3.4 conllu-4.5.2 datasets-2.12.0 deprecated-1.2.13 dill-0.3.6 flair-0.12.2 frozenlist-1.3.3 ftfy-6.1.1 gdown-4.4.0 hyperopt-0.2.7 janome-0.4.2 mpld3-0.3 multidict-6.0.4 multiprocess-0.70.14 pptree-3.1 protobuf-3.20.2 py4j-0.10.9.7 pyarrow-12.0.0 pytorch-revgrad-0.2.0 responses-0.18.0 segtok-1.5.11 sqlitedict-2.1.0 transformer-smaller-training-vocab-0.2.4 wikipedia-api-0.5.8 xxhash-3.2.0 yarl-1.9.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "qiskit-machine-learning 0.6.0 requires dill<0.3.6,>=0.3.4, but you have dill 0.3.6 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a1a5c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
